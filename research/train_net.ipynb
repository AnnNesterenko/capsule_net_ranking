{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train_net.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9gS3cW23U79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ce9110-e3ea-4f22-f121-9acae098d876"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "i9gS3cW23U79",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry5lDxOn322p"
      },
      "source": [
        "import sys\n",
        "import os"
      ],
      "id": "Ry5lDxOn322p",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot21l2VQ3bQF"
      },
      "source": [
        "path = '/content/drive/MyDrive/diploma_new'\n",
        "#for local path = ''\n",
        "sys.path.append(os.path.abspath(path))\n",
        "\n",
        "sys.path.append(os.path.abspath('/content/drive/MyDrive/diploma_new/CapsE'))\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' "
      ],
      "id": "Ot21l2VQ3bQF",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e0ffa5e"
      },
      "source": [
        "from CapsE.prepare_ import *\n",
        "from CapsE.BatchLoader__ import *\n",
        "from CapsE.training_ import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "id": "9e0ffa5e",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuhj9hObbtQf"
      },
      "source": [
        "num_filters_options = [50, 100, 200, 400, 500]\n",
        "learning_rate_options = [5e-6, 1e-5, 5e-5, 1e-4, 5e-4]"
      ],
      "id": "yuhj9hObbtQf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "244f10c3"
      },
      "source": [
        "## ARXIV"
      ],
      "id": "244f10c3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69eee2dc"
      },
      "source": [
        "#### Doc2Vec embeddings"
      ],
      "id": "69eee2dc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee90917b"
      },
      "source": [
        "lst_embeddings_query, lst_embeddings_doc, \\\n",
        "           train_duplets, test_duplets, \\\n",
        "           train_val_duplets, test_val_duplets = get_data_for_net_ARXIV(\n",
        "                                                embeddings_file=f'{path}/data/ARXIV/embeddings_doc2vec.txt', \n",
        "                                                triples_file=f'{path}/data/ARXIV/arxiv_triplets.txt')"
      ],
      "id": "ee90917b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81eb8a38"
      },
      "source": [
        "data_size = len(train_duplets)\n",
        "\n",
        "train_batch = BatchLoaderEcir(train_duplets, train_val_duplets, batch_size=BATCH_SIZE)"
      ],
      "id": "81eb8a38",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "436d06db",
        "outputId": "c8497983-6614-4c18-bbc8-cc0a25e1c961"
      },
      "source": [
        "for learning_rate in learning_rate_options:\n",
        "    for num_filters in num_filters_options:\n",
        "        lsttest, loss = train_model(lst_embeddings_query,\n",
        "                                    lst_embeddings_doc,\n",
        "                                    data_size,\n",
        "                                    train_batch,\n",
        "                                    test_duplets,\n",
        "                                    test_val_duplets,\n",
        "                                    num_filters=num_filters, \n",
        "                                    learning_rate=learning_rate, \n",
        "                                    epochs=50)\n",
        "        np.save(f\"{path}/saved_lsttest/arxiv_doc2vec/{learning_rate}-{num_filters}.npy\", lsttest)\n",
        "        print(f\"Saved lsttest {learning_rate} - {num_filters}\") "
      ],
      "id": "436d06db",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Seting up the main structure\n",
            "Saved lsttest 0.0005 - 400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e50452e0"
      },
      "source": [
        "#### FastText embeddings"
      ],
      "id": "e50452e0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6abcda"
      },
      "source": [
        "lst_embeddings_query, lst_embeddings_doc, \\\n",
        "           train_duplets, test_duplets, \\\n",
        "           train_val_duplets, test_val_duplets = get_data_for_net_ARXIV(\n",
        "                                                        embeddings_file=f'{path}/data/ARXIV/embeddings_fasttext.txt', \n",
        "                                                        triples_file=f'{path}/data/ARXIV/arxiv_triplets.txt')"
      ],
      "id": "ff6abcda",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86d5b773"
      },
      "source": [
        "data_size = len(train_duplets)\n",
        "\n",
        "train_batch = BatchLoaderEcir(train_duplets, train_val_duplets, batch_size=BATCH_SIZE)"
      ],
      "id": "86d5b773",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3psDCgQ1kH0",
        "outputId": "cfc08b48-958b-45b6-d4f7-aaea36f5d43f"
      },
      "source": [
        "for learning_rate in learning_rate_options:\n",
        "    for num_filters in num_filters_options:\n",
        "        lsttest, loss = train_model(lst_embeddings_query,\n",
        "                                    lst_embeddings_doc,\n",
        "                                    data_size,\n",
        "                                    train_batch,\n",
        "                                    test_duplets,\n",
        "                                    test_val_duplets,\n",
        "                                    num_filters=num_filters, \n",
        "                                    learning_rate=learning_rate, \n",
        "                                    epochs=50)\n",
        "        np.save(f\"{path}/saved_lsttest/arxiv_fasttext/{learning_rate}-{num_filters}.npy\", lsttest)\n",
        "        print(f\"Saved lsttest {learning_rate} - {num_filters}\") "
      ],
      "id": "W3psDCgQ1kH0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Seting up the main structure\n",
            "Saved lsttest 0.0005 - 500\n",
            "INFO:tensorflow:Seting up the main structure\n",
            "Saved lsttest 5e-05 - 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1adb28c6"
      },
      "source": [
        "## MIND"
      ],
      "id": "1adb28c6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b053ec8"
      },
      "source": [
        "lst_embeddings_query, lst_embeddings_doc, \\\n",
        "           train_duplets, test_duplets, \\\n",
        "           train_val_duplets, test_val_duplets = get_data_for_net_MIND(\n",
        "                                                  embeddings_file_train=f'{path}/data/MIND/embeddings_doc2vec_train.txt', \n",
        "                                                  embeddings_file_test=f'{path}/data/MIND/embeddings_doc2vec_test.txt',\n",
        "                                                  behaviors_train=f'{path}/data/MIND/behaviors_train.tsv',\n",
        "                                                  behaviors_test=f'{path}/data/MIND/behaviors_test.tsv')"
      ],
      "id": "2b053ec8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0dqQ6f2c7bg"
      },
      "source": [
        "data_size = len(train_duplets)\n",
        "train_batch = BatchLoaderEcir(train_duplets, train_val_duplets, batch_size=BATCH_SIZE)"
      ],
      "id": "o0dqQ6f2c7bg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09c2c092",
        "outputId": "8b158c1f-c491-410c-8665-1110fb4892ef"
      },
      "source": [
        "for learning_rate in learning_rate_options:\n",
        "    for num_filters in num_filters_options:\n",
        "        lsttest, loss = train_model(lst_embeddings_query,\n",
        "                                    lst_embeddings_doc,\n",
        "                                    data_size,\n",
        "                                    train_batch,\n",
        "                                    test_duplets,\n",
        "                                    test_val_duplets,\n",
        "                                    num_filters=num_filters, \n",
        "                                    learning_rate=learning_rate, \n",
        "                                    epochs=100,\n",
        "                                    dataset='MIND')\n",
        "        lsttest_m1 = [i[0] for i in lsttest]\n",
        "        lsttest_m2 = [i[1] for i in lsttest]\n",
        "        np.save(f\"{path}/saved_lsttest/mind_doc2vec/{learning_rate}-{num_filters}-m1.npy\", lsttest_m1)\n",
        "        np.save(f\"{path}/saved_lsttest/mind_doc2vec/{learning_rate}-{num_filters}-m2.npy\", lsttest_m2)\n",
        "        print(f\"Saved lsttest {learning_rate} - {num_filters}\") "
      ],
      "id": "09c2c092",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Seting up the main structure\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYf9i6RcnyuG"
      },
      "source": [
        "FASTTEXT"
      ],
      "id": "HYf9i6RcnyuG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbFzc4qCn0TD"
      },
      "source": [
        "lst_embeddings_query, lst_embeddings_doc, \\\n",
        "           train_duplets, test_duplets, \\\n",
        "           train_val_duplets, test_val_duplets = get_data_for_net_MIND(\n",
        "                                                  embeddings_file_train=f'{path}/data/MIND/embeddings_fasttext_train.txt', \n",
        "                                                  embeddings_file_test=f'{path}/data/MIND/embeddings_fasttext_test.txt',\n",
        "                                                  behaviors_train=f'{path}/data/MIND/behaviors_train.tsv',\n",
        "                                                  behaviors_test=f'{path}/data/MIND/behaviors_test.tsv')"
      ],
      "id": "gbFzc4qCn0TD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6-7Qsh0n6Vk"
      },
      "source": [
        "data_size = len(train_duplets)\n",
        "train_batch = BatchLoaderEcir(train_duplets, train_val_duplets, batch_size=BATCH_SIZE)"
      ],
      "id": "Z6-7Qsh0n6Vk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gh4UN0ruoDbH",
        "outputId": "cbfb668c-c8ef-4ede-ff49-f68bfb7d014e"
      },
      "source": [
        "for learning_rate in learning_rate_options:\n",
        "    for num_filters in num_filters_options:\n",
        "        lsttest, loss = train_model(lst_embeddings_query,\n",
        "                                    lst_embeddings_doc,\n",
        "                                    data_size,\n",
        "                                    train_batch,\n",
        "                                    test_duplets,\n",
        "                                    test_val_duplets,\n",
        "                                    num_filters=num_filters, \n",
        "                                    learning_rate=learning_rate, \n",
        "                                    epochs=100,\n",
        "                                    dataset='MIND')\n",
        "        lsttest_m1 = [i[0] for i in lsttest]\n",
        "        lsttest_m2 = [i[1] for i in lsttest]\n",
        "        np.save(f\"{path}/saved_lsttest/mind_fasttext/{learning_rate}-{num_filters}-m1.npy\", lsttest_m1)\n",
        "        np.save(f\"{path}/saved_lsttest/mind_fasttext/{learning_rate}-{num_filters}-m2.npy\", lsttest_m2)\n",
        "        print(f\"Saved lsttest {learning_rate} - {num_filters}\") "
      ],
      "id": "Gh4UN0ruoDbH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Seting up the main structure\n",
            "Saved lsttest 0.0001 - 400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtOtbOoR54NX"
      },
      "source": [
        "BERT"
      ],
      "id": "rtOtbOoR54NX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meuwmCmP53TG"
      },
      "source": [
        "lst_embeddings_query, lst_embeddings_doc, \\\n",
        "           train_duplets, test_duplets, \\\n",
        "           train_val_duplets, test_val_duplets = get_data_for_net_MIND(\n",
        "                                                  embeddings_file_train=f'{path}/data/MIND/embeddings_BERT_train.txt', \n",
        "                                                  embeddings_file_test=f'{path}/data/MIND/embeddings_BERT_test.txt',\n",
        "                                                  behaviors_train=f'{path}/data/MIND/behaviors_train.tsv',\n",
        "                                                  behaviors_test=f'{path}/data/MIND/behaviors_test.tsv',\n",
        "                                                  SIZE=768)\n",
        "data_size = len(train_duplets)\n",
        "train_batch = BatchLoaderEcir(train_duplets, train_val_duplets, batch_size=BATCH_SIZE)\n",
        "num_filters_options = [200, 400, 500]\n",
        "learning_rate_options = [1e-4]"
      ],
      "id": "meuwmCmP53TG",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wfu2k0g6NLo",
        "outputId": "3afc9d2c-acda-4e12-d73a-6616987fef3b"
      },
      "source": [
        "for learning_rate in learning_rate_options:\n",
        "    for num_filters in num_filters_options:\n",
        "        lsttest, loss = train_model(lst_embeddings_query,\n",
        "                                    lst_embeddings_doc,\n",
        "                                    data_size,\n",
        "                                    train_batch,\n",
        "                                    test_duplets,\n",
        "                                    test_val_duplets,\n",
        "                                    num_filters=num_filters, \n",
        "                                    learning_rate=learning_rate, \n",
        "                                    epochs=100,\n",
        "                                    dataset='MIND')\n",
        "        lsttest_m1 = [i[0] for i in lsttest]\n",
        "        lsttest_m2 = [i[1] for i in lsttest]\n",
        "        np.save(f\"{path}/saved_lsttest/mind_bert/{learning_rate}-{num_filters}-m1.npy\", lsttest_m1)\n",
        "        np.save(f\"{path}/saved_lsttest/mind_bert/{learning_rate}-{num_filters}-m2.npy\", lsttest_m2)\n",
        "        print(f\"Saved lsttest {learning_rate} - {num_filters}\") "
      ],
      "id": "8wfu2k0g6NLo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Seting up the main structure\n",
            "Saved lsttest 0.0001 - 100\n",
            "INFO:tensorflow:Seting up the main structure\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}