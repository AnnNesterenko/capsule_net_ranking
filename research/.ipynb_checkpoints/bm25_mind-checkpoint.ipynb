{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c091aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d940c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/annanesterenko/Desktop/diplom_final'\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a15a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_1 = 2\n",
    "b = 0.75\n",
    "\n",
    "def count_word_freq(q, D):\n",
    "    freq = 0\n",
    "    for word in D:\n",
    "        if word == q:\n",
    "            freq += 1\n",
    "            \n",
    "    return freq\n",
    "\n",
    "def idf(q, docs, count_docs):\n",
    "    n = 0\n",
    "    for doc in docs:\n",
    "        if q in doc:\n",
    "            n += 1\n",
    "            \n",
    "    return np.log((count_docs + 0.5) / (n + 0.5))\n",
    "\n",
    "def bm25_score(Q, D, docs):   \n",
    "    # avg len of docs in search demand\n",
    "    sum_docs_len = 0\n",
    "    count_docs = 0\n",
    "    for doc in docs:\n",
    "        sum_docs_len += len(doc)\n",
    "        count_docs += 1\n",
    "    avg_doc_len = sum_docs_len / count_docs\n",
    "    \n",
    "    score = 0\n",
    "    for q_i in Q:\n",
    "        freq = count_word_freq(q_i, D)\n",
    "        score_q_i = freq * (k_1 + 1) / (freq + k_1 * (1 - b + b * len(D) / avg_doc_len))\n",
    "        score_q_i *= idf(q_i, docs, count_docs)\n",
    "        score += score_q_i\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b005c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CapsE.prepare_3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c5bcdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_texts_query, lst_texts_doc, \\\n",
    "           train_duplets, test_duplets, \\\n",
    "           train_val_duplets, test_val_duplets = get_data_for_net_MIND_plain_text(\n",
    "                                        texts_train=f'{path}/data/MIND/texts_train.txt', \n",
    "                                        texts_test=f'{path}/data/MIND/texts_test.txt',\n",
    "                                        behaviors_train=f'{path}/data/MIND/behaviors_train.tsv',\n",
    "                                        behaviors_test=f'{path}/data/MIND/behaviors_test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2470ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_duplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ce5f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(lst_texts_query, \n",
    "                     lst_texts_doc, \n",
    "                     test_duplets,\n",
    "                     test_val_duplets):\n",
    "    \n",
    "    all_triplets = len(test_duplets)\n",
    "    recall = 0\n",
    "    recall1 = 0\n",
    "    recall2 = 0\n",
    "    recall3 = 0\n",
    "    precision = 0\n",
    "    \n",
    "    ndcg3 = 0\n",
    "    ndcg5 = 0\n",
    "    ndcg10 = 0\n",
    "    \n",
    "    for i in range(len(test_duplets)):\n",
    "        query = lst_texts_query[test_duplets[i][0][0]]\n",
    "        answers = []\n",
    "        \n",
    "        for j in range(10):\n",
    "            ans = lst_texts_doc[test_duplets[i][j][1]]\n",
    "            answers.append(ans)\n",
    "            \n",
    "        scores = []\n",
    "        relevant_cnt = 0\n",
    "        for j in range(10):\n",
    "            score = bm25_score(query, answers[j], answers)\n",
    "            scores.append([score, test_val_duplets[i][j][0]])\n",
    "            relevant_cnt += test_val_duplets[i][j][0]\n",
    "            \n",
    "        scores.sort(key=lambda x: x[0], reverse=True)\n",
    "        scores_relevance = [i[1] for i in scores]\n",
    "        \n",
    "        precision += sum(scores_relevance[:relevant_cnt]) / relevant_cnt\n",
    "        recall += (sum(scores_relevance[:relevant_cnt]) == relevant_cnt)\n",
    "        recall1 += (scores_relevance[0] == 1)\n",
    "        recall2 += (sum(scores_relevance[:2]) == 2)\n",
    "        recall3 += (sum(scores_relevance[:3]) == 3)\n",
    "        \n",
    "        #ndcg@3\n",
    "        ndcg3 += sum([scores_relevance[k-1] / np.log2(k+1) for k in range(1, 3)]) \\\n",
    "            / sum([1/np.log2(k+1) for k in range(1, 6)])\n",
    "        #ndcg@5\n",
    "        ndcg5 += sum([scores_relevance[k-1] / np.log2(k+1) for k in range(1, 6)]) \\\n",
    "            / sum([1/np.log2(k+1) for k in range(1, 6)])\n",
    "        #ndcg@10\n",
    "        ndcg10 += sum([scores_relevance[k-1] / np.log2(k+1) for k in range(1, 11)]) \\\n",
    "            / sum([1/np.log2(k+1) for k in range(1, 6)])\n",
    "        \n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        \n",
    "    return recall / all_triplets, precision / all_triplets, recall1 / all_triplets, \\\n",
    "            recall2 / all_triplets, recall3 / all_triplets, ndcg3 / all_triplets, \\\n",
    "            ndcg5 / all_triplets, ndcg10 / all_triplets,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eee49f69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0145,\n",
       " 0.4158615079365095,\n",
       " 0.42783333333333334,\n",
       " 0.1785,\n",
       " 0.06416666666666666,\n",
       " 0.23669016246261756,\n",
       " 0.4163153944519146,\n",
       " 0.620464382702663)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_quality(lst_texts_query, lst_texts_doc, test_duplets, test_val_duplets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
